{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"garamond\" size=\"18\" color=\"#122DAC\">***Instrumental Variables***</font>\n",
    "## <font face=\"garamond\" size=\"6\" color=\"#122DAC\">*Econ 430*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import random; random.seed(10) # pre-setting seed\n",
    "from scipy import stats\n",
    "# from scipy.stats import norm, chi2, f\n",
    "\n",
    "# For model fitting\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# For instrumental variables\n",
    "from linearmodels import IV2SLS, IVGMM # also has IVGMMCUE, IVLIML\n",
    "# from statsmodels.sandbox.regression import gmm # would use IV2SLS, IVGMM\n",
    "\n",
    "\n",
    "# # For R kernel\n",
    "# # Note: need working copy of R, initiate chunk with `%%R`\n",
    "# import rpy2.ipython\n",
    "# %load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For advanced IV examples: https://github.com/bashtage/linearmodels/blob/main/examples/iv_advanced-examples.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"garamond\" size=\"14\" color=\"#122DAC\">1. Linear Regression with Random *x*'s</font>\n",
    "\n",
    "The key of this chapter: Relax the assumption that the variable $x$ is not\n",
    "random.\n",
    "\n",
    "Modified Simple Regression Assumptions:\n",
    "\n",
    "1. $y_{i}=\\beta _{1}+\\beta _{2}x_{i}+e_{i}$ correctly describes the relationship between $y_{i}$ and $x_{i}$ in the population, where $\\beta_{1}$ and $\\beta_{2}$ are unknown (fixed) parameters and $e_i$ is an unobservable random error term.\n",
    "2. The data pairs $(x_{i},y_{i})$, $i=1,\\ldots , N$, are obtained by *random sampling*. That is, the data pairs are collected from the same population, by a process in which each pair is independent of every other pair. Such data are said to be independent and identically distributed (*iid*).\n",
    "3. The expected value of the error term $e$, *conditional* on the value of $x$, is zero.\n",
    "\n",
    "    * If $E(e|x)=0$, then we can show that it is also true that $x$ and $e$ are uncorrelated, and that $\\text{Cov}(x,e)=0$. Explanatory variables that *are not* correlated with the error term are called *exogenous variables*.\n",
    "    * Conversely, if $x$ and $e$ are correlated, then $\\text{Cov}(x,e)\\neq 0$ and we can show that $E(e|x)\\neq 0$. Explanatory variables that *are* correlated with the error term are called *endogenous variables*.\n",
    "\n",
    "4. In the sample, $x$ must take at least two different values.\n",
    "5. $\\text{Var}(e|x)=\\sigma ^{2}$. The variance of the error term, conditional on any $x$, is a constant $\\sigma ^{2}$.\n",
    "6. The distribution of the error term is normal.\n",
    "\n",
    "---\n",
    "\n",
    "* Assumption 2 states that both $y$ and $x$ are obtained by a sampling process, and thus are random. This is the only one new assumption on our list.\n",
    "* The result that under the classical assumptions, and fixed $x$'s, the least squares estimator is the best linear unbiased estimator, is a finite sample, or a small sample. This means is that the result does not depend on the size of the sample.\n",
    "\n",
    "* Under assumptions 1-6:\n",
    "\n",
    "    1. The least squares estimator is unbiased.\n",
    "    2. The least squares estimator is the best linear unbiased estimator of the regression parameters, and the usual estimator of $\\sigma ^{2}$ is unbiased.\n",
    "    3. The distributions of the least squares estimators, conditional upon the $x$'s, are normal, and their variances are estimated in the usual way.\n",
    "    4. The usual interval estimation and hypothesis testing procedures are valid.\n",
    "\n",
    "\n",
    "* If $x$ is random, as long as the data are obtained by random sampling and the other usual assumptions hold, no changes in our regression methods are required.\n",
    "\n",
    "* For the purposes of a  \"large sample\" analysis of the least squares estimator, it is convenient to replace assumption 3 by:\n",
    "\n",
    "    (3) $E(e)=0$ and $\\text{Cov}(x,e)=0$.\n",
    "\n",
    "\n",
    "Now we can state that under assumptions 1, 2, (3), 4, and 5, the least squares estimators:\n",
    "\n",
    "1. Are consistent.\n",
    "\n",
    "    They converge in probability to the true parameter values as $N\\rightarrow \\infty$.\n",
    "\n",
    "2. Have approximate normal distributions in large samples, whether the errors are normally distributed or not.\n",
    "\n",
    "    Our usual interval estimators and test statistics are valid, if the sample is large.\n",
    "\n",
    "3. If assumption (3) is not true, and in particular if $\\text{Cov}(x,e)\\neq 0$ so that $x$ and $e$ are correlated, then the least squares estimators are inconsistent.\n",
    "\n",
    "    They do not converge to the true parameter values even in very large samples.\n",
    "\n",
    "    None of our usual hypothesis testing or interval estimation procedures are valid.\n",
    "\n",
    "* The statistical consequences of correlation between $x$ and $e$ is that the least squares estimator is biased---and this bias will not disappear no matter how large the sample.\n",
    "\n",
    "* Consequently the least squares estimator is inconsistent when there is correlation between $x$ and $e$.\n",
    "\n",
    "<img src=\"images/L3_fig1.jpg\" alt=\"(a) Correlated predictor and errors; (b) plot of data, true and fitted regression functions\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"garamond\" size=\"14\" color=\"#122DAC\">2. Cases in which *x* and *e* are Correlated</font>\n",
    "\n",
    "* When an explanatory variable and the error term are correlated, the explanatory variable is said to be endogenous.\n",
    "\n",
    "    This term comes from simultaneous equations models: It means *\"determined within the system.\"*\n",
    "\n",
    "* Using this terminology when an explanatory variable is correlated with the regression error, one is said to have an *\"endogeneity problem.\"*\n",
    "\n",
    "* The errors-in-variables problem occurs when an explanatory variable is measured with error.\n",
    "\n",
    "* If we measure an explanatory variable with error, then it is correlated with the error term, and the least squares estimator is inconsistent.\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Let $y$ = annual savings, and let $x^{\\ast }$ = the permanent annual income of a person.\n",
    "\n",
    "* A simple regression model is:\n",
    "$$\n",
    "y_{i}=\\beta _{1}+\\beta _{2}x_{i}^{*}+v_{i}  \n",
    "$$\n",
    "\n",
    "* Current income ($x_i$) is a measure of permanent income, but it does not measure permanent income exactly. It is sometimes called a *proxy variable*.\n",
    "\n",
    "* To capture this feature, let $u_i =$ random disturbance, and define:\n",
    "$$\n",
    "x_{i}=x_{i}^{\\ast }+u_{i}  \n",
    "$$\n",
    "\n",
    "\n",
    "* Substituting $x_{i}^{\\ast }$ from 2 into 1:\n",
    "\n",
    "\\begin{align*}\n",
    "y_{i} &=\\beta_{1}+\\beta _{2}x_{i}^{\\ast }+v_{i}  \\\\\n",
    "&=\\beta_{1}+\\beta _{2}\\left( x_{i}-u_{i}\\right) +v_{i} \\\\\n",
    "&=\\beta_{1}+\\beta _{2}x_{i}+\\left( v_{i}-\\beta _{2}u_{i}\\right) \\\\\n",
    "&=\\beta_{1}+\\beta _{2}x_{i}+e_{i}\n",
    "\\end{align*}\n",
    "\n",
    "* In order to estimate Eq. 3 by least squares, we must determine whether or not $x$ is uncorrelated with the random disturbance $e$.\n",
    "\n",
    "* The covariance between these two random variables, using the fact that $E(e)=0$, is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Cov}\\left(x,e\\right) &=E\\left( xe\\right) =E\\left[ \\left( x^{*}+u\\right)\n",
    "\\left( v-\\beta _{2}u\\right) \\right]   \\\\\n",
    "&=E\\left[ -\\beta _{2}u^{2}\\right] \\\\\n",
    "&=-\\beta _{2}\\sigma _{u}^{2} \\\\\n",
    "&\\neq 0\n",
    "\\end{align*}\n",
    "\n",
    "* The least squares estimator $b_2$ is an *inconsistent* estimator of $\\beta_{2}$ because of the correlation between the explanatory variable and the error term.\n",
    "\n",
    "* Consequently, $b_{2}$ does not converge to $\\beta _{2}$ in large samples (even very large samples).\n",
    "\n",
    "* In large or small samples $b_{2}$ is *not* approximately normal with mean $\\beta _{2}$ and variance:\n",
    "$$\n",
    "Var\\left( b_{2}\\right) =\\frac{\\sigma ^{2}}{\\sum_{i}\\left( x_{i}-\\overline{x}\n",
    "\\right) ^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Simultaneous Regression Model\n",
    "\n",
    "* Another situation in which an explanatory variable is correlated with the regression error term arises in simultaneous equations models (e.g., consider a supply and demand model where the equilibrium price and quantity are estimated simultaneously).\n",
    "\n",
    "* Suppose we write:\n",
    "$$\n",
    "Q=\\beta _{1}+\\beta _{2}P+e  \n",
    "$$\n",
    "\n",
    "* There is a feedback relationship between $P$ and $Q$.\n",
    "* Because of this, which results because price and quantity are jointly, or simultaneously, determined, we can show that $\\text{Cov}(P,e)\\neq 0$.\n",
    "* The resulting bias (and inconsistency) is called the *simultaneous equations bias*.\n",
    "* When an omitted variable is correlated with an included explanatory variable, then the regression error will be correlated with the explanatory variable, making it endogenous.\n",
    "* Consider a log-linear regression model explaining observed hourly wage:\n",
    "\\begin{align*}\n",
    "\\ln \\left( WAGE\\right) =&\\ \\beta _{1}+\\beta _{2}EDUC+\\beta _{3}EXPER \\\\\n",
    "&+\\beta _{4}EXPER^{2}+e\n",
    "\\end{align*}\n",
    "\n",
    "* **Q:** What else affects wages? What have we omitted?\n",
    "* **A:** Ability\n",
    "* We might expect $\\text{Cov}(EDUC,e)\\neq 0$.\n",
    "* If this is true, then we can expect that the least squares estimator of the returns to another year of education will be positively biased, $ E(b_{2})>\\beta_{2}$, and inconsistent.\n",
    "* The bias will not disappear even in very large samples.\n",
    "* Estimating our wage equation, we have:\n",
    "\\begin{align*}\n",
    "\\underset{(se)}{\\ln(WAGE)} =&\\ \\underset{(0.1986)}{-0.522} +\\ \\underset{(0.0141)}{0.1075EDUC} + \\underset{(0.0132)}{0.0416EXPER} \\\\\n",
    "&\\ \\underset{(0.0004)}{-0.0008EXPER^2}\n",
    "\\end{align*}\n",
    "\n",
    "* We estimate that for an additional year of education we expect wages to increase by  approximately 10.75\\%, holding everything else constant.\n",
    "* If ability has a positive effect on wages, then this estimate is overstated, as the contribution of ability is attributed to the education variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.157\n",
      "Model:                            OLS   Adj. R-squared:                  0.151\n",
      "Method:                 Least Squares   F-statistic:                     26.29\n",
      "Date:                Sun, 27 Oct 2024   Prob (F-statistic):           1.30e-15\n",
      "Time:                        15:01:17   Log-Likelihood:                -431.60\n",
      "No. Observations:                 428   AIC:                             871.2\n",
      "Df Residuals:                     424   BIC:                             887.4\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        -0.5220      0.199     -2.628      0.009      -0.912      -0.132\n",
      "educ              0.1075      0.014      7.598      0.000       0.080       0.135\n",
      "exper             0.0416      0.013      3.155      0.002       0.016       0.067\n",
      "I(exper ** 2)    -0.0008      0.000     -2.063      0.040      -0.002   -3.82e-05\n",
      "==============================================================================\n",
      "Omnibus:                       77.792   Durbin-Watson:                   1.961\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              300.917\n",
      "Skew:                          -0.753   Prob(JB):                     4.54e-66\n",
      "Kurtosis:                       6.822   Cond. No.                     2.21e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.21e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mroz = pd.read_csv('data/mroz.csv')\n",
    "mroz1 = mroz[mroz['lfp'] == 1] # lpf = 1 if woman worked in 1975, else = 0\n",
    "\n",
    "# Baseline model\n",
    "mroz_mod = smf.ols('np.log(wage) ~ educ + exper + I(exper**2)', data=mroz1).fit()\n",
    "print(mroz_mod.summary())\n",
    "\n",
    "plt.scatter(x=mroz1['educ'], y=mroz_mod.resid)\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlabel('Years of Education')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.163\n",
      "Model:                            OLS   Adj. R-squared:                  0.155\n",
      "Method:                 Least Squares   F-statistic:                     20.55\n",
      "Date:                Sun, 27 Oct 2024   Prob (F-statistic):           1.73e-15\n",
      "Time:                        15:01:17   Log-Likelihood:                -430.10\n",
      "No. Observations:                 428   AIC:                             870.2\n",
      "Df Residuals:                     423   BIC:                             890.5\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        -0.4702      0.200     -2.346      0.019      -0.864      -0.076\n",
      "educ              0.1176      0.015      7.692      0.000       0.088       0.148\n",
      "exper             0.0415      0.013      3.158      0.002       0.016       0.067\n",
      "I(exper ** 2)    -0.0008      0.000     -2.126      0.034      -0.002   -6.29e-05\n",
      "mothereduc       -0.0183      0.011     -1.723      0.086      -0.039       0.003\n",
      "==============================================================================\n",
      "Omnibus:                       74.996   Durbin-Watson:                   1.939\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              278.334\n",
      "Skew:                          -0.736   Prob(JB):                     3.64e-61\n",
      "Kurtosis:                       6.666   Cond. No.                     2.24e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.24e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Consider the case where we included mother's years of education\n",
    "mroz_mod2 = smf.ols('np.log(wage) ~ educ + exper + I(exper**2) + mothereduc', data=mroz1).fit()\n",
    "print(mroz_mod2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"garamond\" size=\"14\" color=\"#122DAC\">3. Estimators Based on the Method of Moments</font>\n",
    "\n",
    "* When all the usual assumptions of the linear model hold, the method of moments leads to the least squares estimator.\n",
    "* If $x$ is random and *correlated* with the error term, the method of moments leads to an alternative method, called *instrumental variables* estimation, or two-stage least squares (2SLS) estimation, that will work in large samples.\n",
    "* The $k$-th moment of a random variable $Y$ is the expected value of the random variable raised to the $k$-th power:\n",
    "\\begin{equation}\n",
    "E\\left( Y^{k}\\right) =\\mu _{k}=k\\text{-th moment of }Y \n",
    "\\end{equation}\n",
    "* The $k$-th population moment in (1) can be estimated consistently using the sample (of size $N$) analog: \n",
    "$$\n",
    "\\widehat{E}\\left( Y^{k}\\right) =\\widehat{\\mu }_{k}=k\\text{-th sample moment of }Y=\\frac{1}{N}\\sum_{i}y_{i}^{k}  \n",
    "$$\n",
    "\n",
    "* The method of moments estimation procedure equates $m$ population moments to $m$ sample moments to estimate $m$ unknown parameters.\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "Var\\left( Y\\right) =\\sigma ^{2}=E\\left[ \\left( Y-\\mu \\right) ^{2}\\right]=E\\left( Y^{2}\\right) -\\mu ^{2}  \n",
    "$$\n",
    "\n",
    "* The first two population and sample moments of $Y$ are:\n",
    "\\begin{align*}\n",
    "&\\text{Population Moments} && \\phantom{---} \\text{Sample Moments} \\\\ \n",
    "&E\\left( Y\\right) =\\mu _{1}=\\mu && \\phantom{---} \\widehat{\\mu }=\\frac{1}{N} \\sum_{i}y_{i} \\\\ \n",
    "&E\\left( Y^{2}\\right) =\\mu _{2} && \\phantom{---} \\widehat{\\mu }_{2}=\\frac{1}{N} \\sum_{i}y_{i}^{2}\n",
    "\\end{align*}\n",
    "\n",
    "* Solve for the unknown mean and variance parameters: $$\\widehat{\\mu }=\\frac{1}{N}\\sum_{i}y_{i}=\\overline{y}$$ and $$\\widehat{\\sigma }^{2}=\\widehat{\\mu }_{2}-\\widehat{\\mu }^{2} =\\frac{1}{N} \\sum_{i}y_{i}^{2}-\\overline{y}^{2}=\\frac{1}{N}\\sum_{i}\\left( y_{i}-\\overline{y}\\right) ^{2}$$\n",
    "\n",
    "* In the linear regression model we have \n",
    "$$\n",
    "y=\\beta _{1}+\\beta _{2}x+e,\n",
    "$$\n",
    "\n",
    "* We usually assume:\n",
    "\\begin{equation} \\tag{2}\n",
    "E\\left( e_{i}\\right) =0\\Longrightarrow E\\left( y_{i}-\\beta _{1}-\\beta\n",
    "_{2}x_{i}\\right) =0  \n",
    "\\end{equation}\n",
    "\n",
    "* If $x$ is fixed, or random but not correlated with $e$, then:\n",
    "$$\n",
    "E\\left( x_{i}e_{i}\\right) =0\\Longrightarrow E\\left( y_{i}-\\beta _{1}-\\beta_{2}x_{i}\\right) x_{i}=0  \n",
    "$$\n",
    "\n",
    "* We have two equations with two unknown parameters:\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}\\sum_{i}\\left( y_{i}-b_{1}-b_{2}x_{i}\\right) \\phantom{.} &= \\phantom{.} 0 \\\\\n",
    "\\frac{1}{N}\\sum_{i}\\left( y_{i}-b_{1}-b_{2}x_{i}\\right) x_{i} \\phantom{.} &= \\phantom{.} 0\n",
    "\\end{align*}\n",
    "\n",
    "* These are equivalent to the least squares normal equations and their solution is:\n",
    "\\begin{align*}\n",
    "b_{2} &=\\frac{\\sum_{i}\\left( x_{i}-\\overline{x}\\right) \\left( y_{i}-\\overline{y}\\right) }{\\sum_{i}\\left( x_{i}-\\overline{x}\\right) ^{2}} \\\\\n",
    "b_{1} &=\\overline{y}-b_{2}\\overline{x}\n",
    "\\end{align*}\n",
    "\n",
    "* Under \"nice\" assumptions, the method of moments principle of estimation leads us to the same estimators for the simple linear regression model as the least squares principle.\n",
    "* Suppose that there is another variable, $z$, such that:\n",
    "\n",
    "1. $z$ does not have a direct effect on $y$, and thus, it does not belong on the right-hand side of the model as an explanatory variable.\n",
    "2. $z$ is not correlated with the regression error term $e$.\n",
    "3. Variables with this property are said to be \\emph{exogenous variables}.\n",
    "4. $z$ is strongly (or at least not weakly) correlated with $x$, the endogenous explanatory variable.\n",
    "\n",
    "* A variable $z$ with these 4 properties is called an *instrumental variable*.\n",
    "\n",
    "\n",
    "* If such a variable $z$ exists, then it can be used to form the moment condition:\n",
    "\\begin{equation} \\tag{3}\n",
    "E\\left( ze\\right) =0\n",
    "\\Longrightarrow E\\left[ z\\left( y-\\beta _{1}-\\beta_{2}x\\right) \\right] =0  \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* Use equations (2) and (3), the sample moment conditions are:\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}\\sum_{i}\\left( y_{i}-\\widehat{\\beta }_{1}-\\widehat{\\beta }\n",
    "_{2}x_{i}\\right) &=0   \\\\\n",
    "\\frac{1}{N}\\sum_{i}\\left( y_{i}-\\widehat{\\beta }_{1}-\\widehat{\\beta }\n",
    "_{2}x_{i}\\right) z_{i} &=0\n",
    "\\end{align*}\n",
    "\n",
    "* Solving these equations leads us to the method of moments estimators, which are also called, in this case, the *instrumental variable (IV) estimators*:\n",
    "\\begin{align*}\n",
    "\\widehat{\\beta }_{2}\n",
    "&=\\frac{N\\sum_{i}z_{i}y_{i}-\\sum_{i}z_{i}\\sum_{i}y_{i}}{N\\sum_{i}z_{i}x_{i}-\\sum_{i}z_{i}\\sum_{i}x_{i}}   \\\\\n",
    "&=\\frac{\\sum_{i}\\left( z_{i}-\\overline{z}\\right) \\left( y_{i}-\\overline{y} \\right) }{\\sum_{i}\\left( z_{i}-\\overline{z}\\right) \\left( x_{i}-\\overline{x}\\right) } \\\\\n",
    "\\widehat{\\beta }_{2} &=\\overline{y}-\\widehat{\\beta }_{2}\\overline{x}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Properties of the New Estimators\n",
    "\n",
    "* They are consistent, if $z$ is exogenous, with $E(ze)=0$.\n",
    "* In large samples the instrumental variable estimators have approximate normal distributions.\n",
    "* In the simple regression model:\n",
    "$$\n",
    "\\widehat{\\beta }_{2}\\sim N\\left( \\beta _{2},\\frac{\\sigma ^{2}}{r_{zx}^{2}\\sum_{i}\\left( x_{i}-\\overline{x}\\right) ^{2}}\\right)  \n",
    "$$\n",
    "\n",
    "* The error variance is estimated using the estimator:\n",
    "$$\n",
    "\\widehat{\\sigma }_{IV}^{2}=\\frac{1}{N-2}\\sum_{i}\\left( y_{i}-\\widehat{\\beta }_{1}-\\widehat{\\beta }_{2}x_{i}\\right) ^{2}\n",
    "$$\n",
    "\n",
    "* Note that we can write the variance of the instrumental variables estimator of $\\beta _{2}$ as:%\n",
    "$$\n",
    "Var\\left( \\widehat{\\beta }_{2}\\right)\n",
    "=\\frac{\\sigma ^{2}}{r_{zx}^{2}\\sum_{i}\\left( x_{i}-\\overline{x}\\right) ^{2}}\n",
    "=\\frac{Var\\left(b_{2}\\right) }{r_{zx}^{2}}\n",
    "$$\n",
    "\n",
    "* Because $r_{zx}^{2}<1$ the variance of the instrumental variables estimator will always be larger than the variance of the least squares estimator, and thus it is said to be less efficient.\n",
    "\n",
    "\n",
    "\n",
    "* To extend our analysis to a more general setting, consider the multiple regression model:\n",
    "$$\n",
    "y=\\beta _{1}+\\beta _{2}x_{2}+\\cdots +\\beta _{K}x_{K}+e\n",
    "$$\n",
    "\n",
    "* Let $x_{K}$ be an endogenous variable correlated with the error term.\n",
    "* The first $K-1$ variables are exogenous variables that are uncorrelated with the error term $e$--they are called the *\"included\"* instruments.\n",
    "* We can estimate this equation in two steps with a least squares estimation in each step.\n",
    "\n",
    "\n",
    "\n",
    "* The ***first stage regression*** has the endogenous variable $x_{K}$ on the left-hand side, and *all exogenous and instrumental variables* on the right-hand side.\n",
    "\n",
    "* The first stage regression is:\n",
    "$$\n",
    "x_{K}=\\gamma _{1}+\\gamma _{2}x_{2}+\\cdots \\gamma_{K-1}x_{K-1}+\\theta _{1}z_{1}+\\cdots +\\theta _{L}z_{L}+v_{K} \n",
    "$$\n",
    "\n",
    "* The least squares fitted value is:%\n",
    "$$\n",
    "\\widehat{x}_{K}=\\widehat{\\gamma }_{1}+\\widehat{\\gamma }_{2}x_{2}+\\cdots \\widehat{\\gamma }_{K-1}x_{K-1}+\\widehat{\\theta }_{1}z_{1}+\\cdots +\\widehat{\\theta }_{L}z_{L}  \n",
    "$$\n",
    "\n",
    "* The ***second stage regression*** is based on the original specification:\n",
    "$$\n",
    "y=\\beta _{1}+\\beta _{2}x_{2}+\\cdots +\\beta _{K}\\widehat{x}_{K}+e^{*} \n",
    "$$\n",
    "\n",
    "* The least squares estimators from this equation are the *instrumental variables (IV)* estimators.\n",
    "\n",
    "* Because they can be obtained by two least squares regressions, they are also popularly known as the *two-stage least squares (2SLS)* estimators.\n",
    "\n",
    "* The IV/2SLS estimator of the error variance is based on the residuals from the original model:\n",
    "$$\n",
    "\\widehat{\\sigma }_{IV}^{2}\n",
    "=\\frac{\\sum_{i}\\left( y-\\widehat{\\beta }_{1}+\\widehat{\\beta }_{2}x_{2}-\\cdots-\\widehat{\\beta}_{K}x_{K}\\right) ^{2}}{N-K}  \n",
    "$$\n",
    "\n",
    "* In the simple regression, if $x$ is endogenous and we have $L$ instruments:\n",
    "$$\n",
    "\\widehat{x}=\\widehat{\\gamma }_{1}+\\widehat{\\theta }_{1}z_{1}+\\cdots +\\widehat{\\theta}_{L}z_{L}\n",
    "$$\n",
    "\n",
    "* The two sample moment conditions are:\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}\\sum_{i}\\left( y_{i}-\\widehat{\\beta }_{1}-\\widehat{\\beta }_{2}x_{i}\\right) &=0 \\\\\n",
    "\\frac{1}{N}\\sum_{i}\\left( y_{i}-\\widehat{\\beta }_{1}-\\widehat{\\beta }_{2}x_{i}\\right) \\widehat{x}_{i} &=0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* Solving using the fact that $\\overline{\\widehat{x}}=\\overline{x}$, we get:\n",
    "\\begin{align*}\n",
    "\\widehat{\\beta_2}\n",
    "&= \\frac{\\sum_i\\left(\\hat x_i - \\overline{\\hat x}\\right)(y_i-\\bar y)}\n",
    "{\\sum_i\\left(\\hat x_i - \\overline{\\hat x}\\right)(x_i-\\bar x)} \\\\\n",
    "\\widehat{\\beta_1}&= \\bar y - \\widehat{\\beta_2}\\bar x\n",
    "\\end{align*}\n",
    "\n",
    "* Sometimes we have more instrumental variables at our disposal than are necessary.\n",
    "* Suppose we have $L=2$ instruments, $z_{1}$ and $z_{2}$.\n",
    "* Then we have:\n",
    "$$\n",
    "E\\left( z_{2}e\\right) =E\\left[ z_{2}\\left( y-\\beta _{1}-\\beta _{2}x\\right)\\right] =0\n",
    "$$\n",
    "\n",
    "* We have three sample moment conditions:\n",
    "\\begin{align*}\n",
    "&\\frac1N\\sum_i\\left(y_i - \\widehat{\\beta_1}-\\widehat{\\beta_2}x_i\\right) &&= \\widehat m_1 = 0 \\\\\n",
    "&\\frac1N\\sum_i\\left(y_i - \\widehat{\\beta_1}-\\widehat{\\beta_2}x_i\\right)z_{i1} &&= \\widehat m_2 = 0 \\\\\n",
    "&\\frac1N\\sum_i\\left(y_i - \\widehat{\\beta_1}-\\widehat{\\beta_2}x_i\\right)z_{i2} &&= \\widehat m_3 = 0\n",
    "\\end{align*}\n",
    "\n",
    "* The first stage regression is a key tool in assessing whether an instrument is \"strong\" or \"weak\" in the multiple regression setting.\n",
    "* Suppose the first stage regression equation is:\n",
    "$$\n",
    "x_{K}=\\gamma _{1}+\\gamma _{2}x_{2}+\\cdots +\\gamma_{K-1}x_{K-1}+\\theta _{1}z_{1}+v_{K}  \n",
    "$$\n",
    "\n",
    "* The key to assessing the strength of the instrumental variable $z_{1}$ is the strength of its relationship to $x_{K}$ after controlling for the effects of all the other exogenous variables.\n",
    "\n",
    "* Suppose the first stage regression equation is:\n",
    "$$\n",
    "x_{K}=\\gamma _{1}+\\gamma _{2}x_{2}+\\cdots +\\gamma_{K-1}x_{K-1}+\\theta _{1}z_{1}+\\cdots \\theta _{L}z_{L}+v_{K} \n",
    "$$\n",
    "\n",
    "* We require that *at least* one of the instruments be strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Example:**\n",
    "\n",
    "*Wage Equation with IV = mother's years of education.*\n",
    "\n",
    "* Consider the model with an instrumental variable $MOTHEREDUC$:\n",
    "\\begin{align*}\n",
    "\\underset{(se)}{\\widehat{EDUC}}\n",
    "&= \\underset{(0.4249)}{9.7751} + \\underset{(0.0427)}{0.0489}EXPER - \\underset{(0.0012)}{0.0013}EXPER^2 \\\\\n",
    "&\\phantom{-}+ \\underset{(0.0311)}{0.2677}MOTHEREDUC\n",
    "\\end{align*}\n",
    "\n",
    "* To implement instrumental variables estimation using the two-stage least squares approach, we obtain the predicted values of education from the first stage equation and insert it into the log-linear wage equation to\n",
    "replace $EDUC$.\n",
    "* Then estimate the resulting equation by least squares.\n",
    "\n",
    "\n",
    "* The instrumental variables estimates of the log-linear wage equation\n",
    "are:\n",
    "\\begin{align*}\n",
    "\\ln(\\underset{(se)}{\\widehat{WAGE}})\n",
    "&= \\underset{(0.4729)}{0.1982} + \\underset{(0.0374)}{0.0493}EDUC + \\underset{(0.0136)}{0.0449}EXPER \\\\\n",
    "&\\phantom{-}- \\underset{(0.0004)}{0.0009}EXPER^2\n",
    "\\end{align*}\n",
    "\n",
    "* Note: As we can see, the coefficient  of EDUC is now 4.93\\% compared to our OLS estimate of 10.75\\%.\n",
    "\n",
    "* Using $FATHEREDUC$, the first stage equation is:\n",
    "\\begin{align*}\n",
    "EDUC &= \\gamma_1 + \\gamma_2 EXPER + \\gamma_3 EXPER^2 + \\theta_1 MOTHEREDUC \\\\\n",
    "&\\phantom{-}+\\theta_2 FATHEREDUC + v_k\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* The IV/2SLS estimates are:\n",
    "\\begin{align*}\n",
    "\\ln(\\underset{(se)}{\\widehat{WAGE}})\n",
    "&= \\underset{(0.4003)}{0.481} + \\underset{(0.0374)}{0.0614}EDUC + \\underset{(0.0442)}{0.0136}EXPER \\\\\n",
    "&\\phantom{-}- \\underset{(0.0004)}{0.0009}EXPER^2\n",
    "\\end{align*}\n",
    "\n",
    "* Note: In this case the coefficient of EDUC is now 6.14\\% compared to our OLS estimate of 10.75\\%.\n",
    "\n",
    "\n",
    "* In a multiple regression model, the coefficients are the effect of a unit change in an explanatory, independent, variable on the expected outcome, holding all other things constant.\n",
    "* In calculus terminology, the coefficients are partial derivatives.\n",
    "\n",
    "* We can net out or partial out the effects of explanatory variables.\n",
    "    * Regression coefficients can be thought of measuring the effect of one variable on another after removing, or partialling out, the effects of all other variables.\n",
    "    * The sample correlation between two residuals is called the *partial correlation* coefficient.\n",
    "* The multiple regression model, including all $m$ variables, is:\n",
    "\n",
    "$$\n",
    "y=\n",
    "\\overset{G\\text{ exogenous variables}}{\\overbrace{\\beta _{1}+\\beta_{2}x_{2}+\\cdots +\\beta _{G}x_{G}}}\n",
    "+\\overset{B\\text{ endogenous variables}}{\\overbrace{\\beta_{G+1}x_{_{G+1}}+\\cdots +\\beta_{K}x_{K}}}\n",
    "+e\n",
    "$$\n",
    "\n",
    "* Think of $G$ = Good explanatory variables, $B$ = Bad explanatory variables and $L$ = Lucky instrumental variables.\n",
    "* It is a necessary condition for IV estimation that $L\\geq B$.\n",
    "* If $L=B$ then there are just enough instrumental variables to carry out IV estimation.\n",
    "\n",
    "    The model parameters are said to *just identified* or *exactly identified* in this case.\n",
    "\n",
    "* The term identified is used to indicate that the model parameters can be consistently estimated.\n",
    "* If $L>B$ then we have more instruments than are necessary for IV estimation, and the model is said to be *overidentified*.\n",
    "\n",
    "* Consider the $B$ first-stage equations: $$x_{G+j}=\\gamma _{1j}+\\gamma _{2j}x_{2}+\\cdots +\\gamma_{Gj}x_{G}+\\theta _{1j}z_{1}+\\cdots +\\theta _{Lj}z_{L}+v_{j}, $$ for $j=1,\\dots,B$\n",
    "\n",
    "* The predicted values are:$$\\widehat{x}_{G+j}=\\widehat{\\gamma }_{1j}+\\widehat{\\gamma }_{2j}x_{2}+\\cdots +\\widehat{\\gamma}_{Gj}x_{G}+\\widehat{\\theta}_{1j}z_{1}+\\cdots +\\widehat{\\theta }_{Lj}z_{L}$$\n",
    "\n",
    "* In the second stage of estimation we apply least squares to:\n",
    "$$\n",
    "y=\\beta _{1}+\\beta _{2}x_{2}+\\cdots +\\beta _{G}x_{G}+\\beta _{G+1} \\widehat{x}_{_{G+1}}+\\cdots +\\beta _{K}\\widehat{x}_{G+B}+e^{*}\n",
    "$$\n",
    "\n",
    "\n",
    "* Consider the model with $B=2$:\n",
    "$$\n",
    "y=\\beta _{1}+\\beta _{2}x_{2}+\\cdots +\\beta _{G}x_{G}+\\beta_{G+1}x_{G+1}+\\beta _{G+2}x_{G+2}+e  \n",
    "$$\n",
    "\n",
    "* The first-stage equations are:\n",
    "\\begin{align*}\n",
    "x_{G+1}&    \\\\\n",
    "& =\\gamma _{11}+\\gamma _{21}x_{2}+\\cdots +\\gamma_{G1}x_{G}+\\theta _{11}z_{1}+\\theta _{21}z_{2}+v_{1}, \\\\\n",
    "x_{G+2}&   \\\\\n",
    "& =\\gamma _{12}+\\gamma _{22}x_{2}+\\cdots +\\gamma_{G2}x_{G}+\\theta _{12}z_{1}+\\theta _{22}z_{2}+v_{2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* When testing the null hypothesis $H_{0}$: $\\beta _{k}=c$, use of the\n",
    "test statistic $$t=\\frac{\\left( \\widehat{\\beta }_{k}-c\\right) }{se\\left( \\widehat{\\beta }_{k}\\right) }$$ is valid in large samples.\n",
    "* It is common, but not universal, practice to use critical values, and $p$-values, based on the $t$-distribution rather than the more strictly appropriate $N(0,1)$ distribution.\n",
    "* The reason is that tests based on the $t$-distribution tend to work\n",
    "better in samples of data that are not large.\n",
    "\n",
    "* When testing a joint hypothesis, such as $H_{0}$: $\\beta _{2}=c_{2},\\ \\beta _{3}=c_{3}$, the test may be based on the chi-square distribution with the number of degrees of freedom equal to the number of hypotheses ($J$) being tested.\n",
    "    * The test itself may be called a \"Wald\" test, or a likelihood ratio (*LR*) test, or a Lagrange multiplier (*LM*) test\n",
    "    * These testing procedures are all asymptotically equivalent.\n",
    "\n",
    "\n",
    "## 3.2 A word on R-squared\n",
    "\n",
    "* Unfortunately $R^{2}$ can be negative when based on IV estimates.\n",
    "* Therefore the use of measures like $R^{2}$ outside the context of the least squares estimation should be avoided.\n",
    "\n",
    "## 3.3 IV in Python\n",
    "\n",
    "Using the `linearmodels` library, you can specify how to fit the model in two ways. Similar to OLS in `statsmodels`, you can either specify the model formulaically or by passing data frames into the function. To pass the data explicitly:\n",
    "\n",
    "```python\n",
    "dependent = data[dependent_var]\n",
    "exog = sm.add_constant(data[exog_vars])                 # must add constant\n",
    "endog = data[endogenous_vars]\n",
    "instruments = data[instruments]\n",
    "\n",
    "model = IV2SLS(dependent, exog, endog, instruments)\n",
    "fitted_model = mod.fit(cov_type='unadjusted')           # can specify 'robust' or other corrections\n",
    "```\n",
    "\n",
    "With the above method if any one of the arguments is not part of the model (say, endogenous variables), then the argument should be passed `None`. To fit a model based off a formula, it must use the form `dep ~ 1 + exog + [endog ~ instruments]`. Note that `1` adds the constant. We observe the formula method below:\n",
    "\n",
    "```python\n",
    "formula = (\n",
    "    'y ~ 1 + x_2 + ... + x_G + [x_G1 + ... + x_K ~ z_1 + ... + z_L]'\n",
    ")\n",
    "model = IV2SLS.from_formula(formula, data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.157\n",
      "Model:                            OLS   Adj. R-squared:                  0.151\n",
      "Method:                 Least Squares   F-statistic:                     26.29\n",
      "Date:                Sun, 27 Oct 2024   Prob (F-statistic):           1.30e-15\n",
      "Time:                        15:01:17   Log-Likelihood:                -431.60\n",
      "No. Observations:                 428   AIC:                             871.2\n",
      "Df Residuals:                     424   BIC:                             887.4\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        -0.5220      0.199     -2.628      0.009      -0.912      -0.132\n",
      "educ              0.1075      0.014      7.598      0.000       0.080       0.135\n",
      "exper             0.0416      0.013      3.155      0.002       0.016       0.067\n",
      "I(exper ** 2)    -0.0008      0.000     -2.063      0.040      -0.002   -3.82e-05\n",
      "==============================================================================\n",
      "Omnibus:                       77.792   Durbin-Watson:                   1.961\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              300.917\n",
      "Skew:                          -0.753   Prob(JB):                     4.54e-66\n",
      "Kurtosis:                       6.822   Cond. No.                     2.21e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.21e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Again using modified 'mroz' data\n",
    "\n",
    "# Baseline\n",
    "mroz_mod3 = smf.ols('np.log(wage) ~ educ + exper + I(exper**2)', data=mroz1).fit()\n",
    "print(mroz_mod3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                      0.1231\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1169\n",
      "No. Observations:                 428   F-statistic:                    16.662\n",
      "Date:                Sun, Oct 27 2024   P-value (F-stat)                0.0008\n",
      "Time:                        15:01:17   Distribution:                  chi2(3)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                              Parameter Estimates                              \n",
      "===============================================================================\n",
      "             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       0.1982     0.4869     0.4071     0.6840     -0.7560      1.1524\n",
      "exper           0.0449     0.0155     2.8882     0.0039      0.0144      0.0753\n",
      "I(exper**2)    -0.0009     0.0004    -2.1451     0.0319     -0.0018  -7.957e-05\n",
      "educ            0.0493     0.0379     1.3011     0.1932     -0.0249      0.1235\n",
      "===============================================================================\n",
      "\n",
      "Endogenous: educ\n",
      "Instruments: mothereduc\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# One instrument = mother's years of education\n",
    "f1 = 'np.log(wage) ~ 1 + exper + I(exper**2) + [educ ~ mothereduc]'\n",
    "mroz_iv1 = IV2SLS.from_formula(f1, data=mroz1).fit(cov_type='robust')\n",
    "print(mroz_iv1.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                      0.1430\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1370\n",
      "No. Observations:                 428   F-statistic:                    19.229\n",
      "Date:                Sun, Oct 27 2024   P-value (F-stat)                0.0002\n",
      "Time:                        15:01:17   Distribution:                  chi2(3)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                              Parameter Estimates                              \n",
      "===============================================================================\n",
      "             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept      -0.0611     0.4560    -0.1340     0.8934     -0.9548      0.8326\n",
      "exper           0.0437     0.0155     2.8187     0.0048      0.0133      0.0740\n",
      "I(exper**2)    -0.0009     0.0004    -2.0552     0.0399     -0.0017   -4.09e-05\n",
      "educ            0.0702     0.0358     1.9632     0.0496      0.0001      0.1403\n",
      "===============================================================================\n",
      "\n",
      "Endogenous: educ\n",
      "Instruments: fathereduc\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# One instrument = father's years of education\n",
    "f2 = 'np.log(wage) ~ 1 + exper + I(exper**2) + [educ ~ fathereduc]'\n",
    "mroz_iv2 = IV2SLS.from_formula(f2, data=mroz1).fit(cov_type='robust')\n",
    "print(mroz_iv2.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                      0.1357\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1296\n",
      "No. Observations:                 428   F-statistic:                    18.611\n",
      "Date:                Sun, Oct 27 2024   P-value (F-stat)                0.0003\n",
      "Time:                        15:01:17   Distribution:                  chi2(3)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                              Parameter Estimates                              \n",
      "===============================================================================\n",
      "             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       0.0481     0.4278     0.1124     0.9105     -0.7903      0.8865\n",
      "exper           0.0442     0.0155     2.8546     0.0043      0.0138      0.0745\n",
      "I(exper**2)    -0.0009     0.0004    -2.1001     0.0357     -0.0017  -5.997e-05\n",
      "educ            0.0614     0.0332     1.8503     0.0643     -0.0036      0.1264\n",
      "===============================================================================\n",
      "\n",
      "Endogenous: educ\n",
      "Instruments: mothereduc, fathereduc\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# Two instruments = father's and mother's years of education\n",
    "f3 = 'np.log(wage) ~ 1 + exper + I(exper**2) + [educ ~ mothereduc + fathereduc]'\n",
    "mroz_iv3 = IV2SLS.from_formula(f3, data=mroz1).fit(cov_type='robust')\n",
    "print(mroz_iv3.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"garamond\" size=\"14\" color=\"#122DAC\">4. Specification Tests</font>\n",
    "\n",
    "To start, the first test we should do is the \"weak instrument\" test. We want our instruments to be strongly correlated with the endogenous variable(s); as a result, a large F-statistic of the first-stage regression indicates good strength. As a rule of thumb, instruments are weak if the first-stage F-statistic is less than 10, otherwise the instruments are strong, and we can proceed.\n",
    "\n",
    "Now let's look at the other tests one might perform.\n",
    "\n",
    "1. Can we test for whether $x$ is correlated with the error term? If we can it might give us a guide of when to use least squares and when to use IV estimators.\n",
    "2. Can we test if our instrument is valid, and uncorrelated with the regression error, as required?\n",
    "\n",
    "* The null hypothesis is $H_{0}$: $Cov(x,e)=0$ against the alternative $H_{1}$: $Cov(x,e)\\neq 0$\n",
    "* If null hypothesis is true, then both the least squares estimator and the instrumental variables estimator are consistent.\n",
    "* If the null hypothesis is false, the least squares estimator is not consistent, and the instrumental variables estimator is consistent.\n",
    "* Comments:\n",
    "    * Naturally if the null hypothesis is true, then one should use the more efficient estimator, which is the least squares estimator.\n",
    "    * If the null hypothesis is not true, then the only consistent estimator is the instrumental variables estimator.\n",
    "* There are several forms of the test, usually called the *Hausman test*.\n",
    "\n",
    "## Hausman test - Step 1\n",
    "\n",
    "* Consider the model:\n",
    "$$\n",
    "y=\\beta _{1}+\\beta _{2}x+e\n",
    "$$\n",
    "* Let $z_{1}$ and $z_{2}$ be instrumental variables for $x$.\n",
    "* Estimate the regression model $$x=\\gamma _{1}+\\theta _{1}z_{1}+\\theta _{2}z_{2}+v$$ by least squares, and obtain the residuals\n",
    "$$\n",
    "\\widehat{v}=x-\\widehat{\\gamma }_{1}+\\widehat{\\theta }_{1}z_{1}+\\widehat{\\theta }_{2}z_{2}+\n",
    "$$\n",
    "\n",
    "* If there are more than one explanatory variables that are being tested for endogeneity, repeat this estimation for each one, using all available instrumental variables in each regression. \n",
    "\n",
    "## Hausman test - Step 2\n",
    "\n",
    "* Include the residuals computed in step 1 as an explanatory variable in the original regression,\n",
    "$$\n",
    "y=\\beta _{1}+\\beta _{2}x+\\delta \\widehat{v}+e\n",
    "$$\n",
    "\n",
    "* Estimate this \"artificial regression\" by least squares, and employ the usual $t$-test for the hypothesis of significance\n",
    "\\begin{align*}\n",
    "H_0&:\\phantom{-} \\delta=0\\phantom{-} \\text{i.e., no correlation between }x\\text{ and }e \\\\\n",
    "H_1&:\\phantom{-} \\delta\\neq0\\phantom{-} \\text{i.e., correlation between }x\\text{ and }e\n",
    "\\end{align*}\n",
    "\n",
    "## Hausman test - Step 3\n",
    "\n",
    "* Note that if more than one variable is being tested for endogeneity, the test will be an $F$-test of joint significance of the coefficients on the included residuals.\n",
    "\n",
    "\n",
    "## A test of the validity of the surplus moment conditions:\n",
    "\n",
    "* **Step 1:** Compute the IV estimates $\\widehat{\\beta }_{k}$ using all available instruments, including the $G$ variables, $x_{1}=1,x_{2},\\ldots ,x_{G}$, that are presumed to be exogenous, and the $L$ instruments.\n",
    "\n",
    "* **Step 2:** Obtain the residuals\n",
    "$$\n",
    "\\widehat{e}=y-\\widehat{\\beta }_{1}-\\widehat{\\beta }_{2}x_{2}-\\cdots -\\widehat{\\beta }_{K}x_{K}\n",
    "$$\n",
    "\n",
    "* **Step 3:** Regress $\\widehat{e}$ on all the available instruments described in step 1.\n",
    "\n",
    "\n",
    "* **Step 4:** Compute $N\\times R^{2}$ from this regression, where $N$ is the sample size and $R^{2}$ is the usual goodness-of-fit measure.\n",
    "\n",
    "* **Step 5:** If all of the surplus moment conditions are valid, then $$N\\times R^{2}\\sim \\chi _{\\left( L-B\\right) }^{2}$$ If the value of the test statistic exceeds the $100(1-\\alpha)$-percentile from the $\\chi _{\\left( L-B\\right) }^{2}$ distribution, then we conclude that at least one of the surplus moment conditions restrictions is not valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                      0.1357\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1296\n",
      "No. Observations:                 428   F-statistic:                    24.653\n",
      "Date:                Sun, Oct 27 2024   P-value (F-stat)                0.0000\n",
      "Time:                        15:01:17   Distribution:                  chi2(3)\n",
      "Cov. Estimator:            unadjusted                                         \n",
      "                                                                              \n",
      "                              Parameter Estimates                              \n",
      "===============================================================================\n",
      "             Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-------------------------------------------------------------------------------\n",
      "Intercept       0.0481     0.3985     0.1207     0.9039     -0.7329      0.8291\n",
      "exper           0.0442     0.0134     3.3038     0.0010      0.0180      0.0704\n",
      "I(exper**2)    -0.0009     0.0004    -2.2485     0.0245     -0.0017     -0.0001\n",
      "educ            0.0614     0.0313     1.9622     0.0497   7.043e-05      0.1227\n",
      "===============================================================================\n",
      "\n",
      "Endogenous: educ\n",
      "Instruments: mothereduc, fathereduc\n",
      "Unadjusted Covariance (Homoskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# Two instruments: mother's and father's years of education\n",
    "f4 = 'np.log(wage) ~ 1 + exper + I(exper**2) + [educ ~ mothereduc + fathereduc]'\n",
    "mroz_iv4 = IV2SLS.from_formula(f4, data=mroz1).fit(cov_type='unadjusted')\n",
    "print(mroz_iv4.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    First Stage Estimation Results    \n",
      "======================================\n",
      "                                  educ\n",
      "--------------------------------------\n",
      "R-squared                       0.2115\n",
      "Partial R-squared               0.2076\n",
      "Shea's R-squared                0.2076\n",
      "Partial F-statistic             56.055\n",
      "P-value (Partial F-stat)      1.11e-16\n",
      "Partial F-stat Distn          F(2,423)\n",
      "========================== ===========\n",
      "Intercept                       9.1026\n",
      "                              (21.465)\n",
      "exper                           0.0452\n",
      "                              (1.1302)\n",
      "I(exper**2)                    -0.0010\n",
      "                             (-0.8435)\n",
      "mothereduc                      0.1576\n",
      "                              (4.4165)\n",
      "fathereduc                      0.1895\n",
      "                              (5.6483)\n",
      "--------------------------------------\n",
      "\n",
      "T-stats reported in parentheses\n",
      "T-stats use same covariance type as original model\n"
     ]
    }
   ],
   "source": [
    "# First stage results (for weak instruments test)\n",
    "\n",
    "print(mroz_iv4.first_stage)\n",
    "# H0: instruments are weak (cutoff is 10)\n",
    "# H1: at least one instrument is strong\n",
    "\n",
    "### RESULT: partial F-statistic = 56.055 > 10, reject H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu-Hausman test of exogeneity\n",
      "H0: All endogenous variables are exogenous\n",
      "Statistic: 2.8035\n",
      "P-value: 0.0948\n",
      "Distributed: F(1,423) \n",
      "----------\n",
      "Sargan's test of overidentification\n",
      "H0: The model is not overidentified.\n",
      "Statistic: 0.3781\n",
      "P-value: 0.5386\n",
      "Distributed: chi2(1)\n"
     ]
    }
   ],
   "source": [
    "# Hausman test\n",
    "print(mroz_iv4.wu_hausman(), '\\n----------')\n",
    "# H0: corr(x, e) = 0\n",
    "# H1: endogeneity\n",
    "\n",
    "# Sargan test\n",
    "print(mroz_iv4.sargan)\n",
    "# H0: extra instruments are valid (i.e., corr(zi, e) = 0)\n",
    "# H1: extra instruments are not valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Code Example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cigarette consumption (data from 1995)\n",
    "\n",
    "# P = is the after-tax average real price per pack of cigarettes in state\n",
    "# Q = number of cigarette packs per capita sold\n",
    "# Income = real per capita income\n",
    "\n",
    "cig = pd.read_csv('data/cigarettesSW.csv')\n",
    "cig['rincome'] = cig['income'] / cig['population'] / cig['cpi']     # real income\n",
    "cig['rprice'] = cig['price'] / cig['cpi']                           # real price\n",
    "cig['salestax'] = (cig['taxs'] - cig['tax']) / cig['cpi']           # real sales tax\n",
    "cig['cigtax'] = cig['tax'] / cig['cpi']                             # real cigarette tax\n",
    "\n",
    "c1995 = cig.copy()[cig['year'] == 1995]     # subset to year = 1995\n",
    "c1995.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          np.log(packs)   R-squared:                       0.447\n",
      "Model:                            OLS   Adj. R-squared:                  0.409\n",
      "Method:                 Least Squares   F-statistic:                     13.73\n",
      "Date:                Sun, 27 Oct 2024   Prob (F-statistic):           1.86e-06\n",
      "Time:                        15:01:17   Log-Likelihood:                 14.434\n",
      "No. Observations:                  48   AIC:                            -20.87\n",
      "Df Residuals:                      44   BIC:                            -13.38\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          11.3236      1.250      9.057      0.000       8.873      13.774\n",
      "np.log(rprice)     -1.6705      0.328     -5.088      0.000      -2.314      -1.027\n",
      "np.log(rincome)     0.4197      0.261      1.605      0.108      -0.093       0.932\n",
      "salestax            0.0144      0.013      1.086      0.277      -0.012       0.041\n",
      "==============================================================================\n",
      "Omnibus:                        6.312   Durbin-Watson:                   1.889\n",
      "Prob(Omnibus):                  0.043   Jarque-Bera (JB):                5.501\n",
      "Skew:                          -0.587   Prob(JB):                       0.0639\n",
      "Kurtosis:                       4.172   Cond. No.                         423.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "\n",
      "test: salestax = 0\n",
      " <F test: F=1.179890085462374, p=0.2832935895398964, df_denom=44, df_num=1>\n"
     ]
    }
   ],
   "source": [
    "# Estimate model w/ IV = 'salestax'\n",
    "cig_m0 = smf.ols('np.log(packs) ~ np.log(rprice) + np.log(rincome) + salestax', data=c1995).fit(cov_type='HC1')\n",
    "print(cig_m0.summary())\n",
    "\n",
    "# Check instrument relevance for 'salestax'\n",
    "print('\\ntest: salestax = 0\\n', cig_m0.f_test([0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          np.log(packs)   R-squared:                       0.443\n",
      "Model:                            OLS   Adj. R-squared:                  0.405\n",
      "Method:                 Least Squares   F-statistic:                     13.17\n",
      "Date:                Sun, 27 Oct 2024   Prob (F-statistic):           2.87e-06\n",
      "Time:                        15:01:17   Log-Likelihood:                 14.291\n",
      "No. Observations:                  48   AIC:                            -20.58\n",
      "Df Residuals:                      44   BIC:                            -13.10\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          12.8102      2.794      4.585      0.000       7.334      18.286\n",
      "np.log(rprice)     -1.9827      0.707     -2.803      0.005      -3.369      -0.596\n",
      "np.log(rincome)     0.3523      0.262      1.346      0.178      -0.161       0.865\n",
      "cigtax              0.0075      0.008      0.962      0.336      -0.008       0.023\n",
      "==============================================================================\n",
      "Omnibus:                        4.919   Durbin-Watson:                   2.037\n",
      "Prob(Omnibus):                  0.085   Jarque-Bera (JB):                3.769\n",
      "Skew:                          -0.550   Prob(JB):                        0.152\n",
      "Kurtosis:                       3.822   Cond. No.                     4.06e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "[2] The condition number is large, 4.06e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "test: cigtax = 0\n",
      " <F test: F=0.9245952212717052, p=0.34152603824621497, df_denom=44, df_num=1>\n"
     ]
    }
   ],
   "source": [
    "# Estimate model w/ IV = 'cigtax'\n",
    "cig_m1 = smf.ols('np.log(packs) ~ np.log(rprice) + np.log(rincome) + cigtax', data=c1995).fit(cov_type='HC1')\n",
    "print(cig_m1.summary())\n",
    "\n",
    "# Check instrument relevance for 'cigtax'\n",
    "print('\\ntest: cigtax = 0\\n', cig_m1.f_test([0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          np.log(packs)   R-squared:                       0.474\n",
      "Model:                            OLS   Adj. R-squared:                  0.425\n",
      "Method:                 Least Squares   F-statistic:                     12.94\n",
      "Date:                Sun, 27 Oct 2024   Prob (F-statistic):           5.34e-07\n",
      "Time:                        15:01:17   Log-Likelihood:                 15.637\n",
      "No. Observations:                  48   AIC:                            -21.27\n",
      "Df Residuals:                      43   BIC:                            -11.92\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          16.2505      3.408      4.769      0.000       9.571      22.930\n",
      "np.log(rprice)     -2.8424      0.873     -3.257      0.001      -4.553      -1.132\n",
      "np.log(rincome)     0.4815      0.278      1.734      0.083      -0.063       1.026\n",
      "salestax            0.0234      0.014      1.614      0.107      -0.005       0.052\n",
      "cigtax              0.0131      0.008      1.568      0.117      -0.003       0.030\n",
      "==============================================================================\n",
      "Omnibus:                        3.785   Durbin-Watson:                   2.021\n",
      "Prob(Omnibus):                  0.151   Jarque-Bera (JB):                2.668\n",
      "Skew:                          -0.463   Prob(JB):                        0.263\n",
      "Kurtosis:                       3.691   Cond. No.                     5.18e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "[2] The condition number is large, 5.18e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "test: salestax = 0, cigtax = 0 \n",
      " <F test: F=1.8519185774553495, p=0.1692355832924754, df_denom=43, df_num=2>\n"
     ]
    }
   ],
   "source": [
    "# Estimate model w/ IV1 = 'salestax', IV2 = 'cigtax'\n",
    "cig_m2 = smf.ols('np.log(packs) ~ np.log(rprice) + np.log(rincome) + salestax + cigtax', data=c1995).fit(cov_type='HC1')\n",
    "print(cig_m2.summary())\n",
    "\n",
    "# Check instrument relevance for 'salestax', 'cigtax'\n",
    "print('\\ntest: salestax = 0, cigtax = 0 \\n', cig_m2.f_test([[0, 0, 0, 1, 0],\n",
    "                                                            [0, 0, 0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:          np.log(packs)   R-squared:                      0.4189\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.3931\n",
      "No. Observations:                  48   F-statistic:                    17.474\n",
      "Date:                Sun, Oct 27 2024   P-value (F-stat)                0.0002\n",
      "Time:                        15:01:17   Distribution:                  chi2(2)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                                Parameter Estimates                                \n",
      "===================================================================================\n",
      "                 Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           9.4307     1.2194     7.7338     0.0000      7.0407      11.821\n",
      "np.log(rincome)     0.2145     0.3018     0.7107     0.4773     -0.3771      0.8061\n",
      "np.log(rprice)     -1.1434     0.3605    -3.1718     0.0015     -1.8499     -0.4368\n",
      "===================================================================================\n",
      "\n",
      "Endogenous: np.log(rprice)\n",
      "Instruments: salestax\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "# Estimate IV model w/ IV1 = 'salestax', IV2 = 'cigtax'\n",
    "\n",
    "cig_iv = IV2SLS.from_formula('np.log(packs) ~ 1 + np.log(rincome) + [np.log(rprice) ~ salestax]',\n",
    "                             data=c1995).fit(cov_type='robust')\n",
    "print(cig_iv.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: salestax = 0, cigtax = 0 \n",
      " <Wald test (chi2): statistic=0.5553747871542599, p-value=0.7575335943711898, df_denom=2>\n"
     ]
    }
   ],
   "source": [
    "# Test validity of instruments (J-statistic) via\n",
    "# overidentifying restrictions test\n",
    "\n",
    "cig_iv_OR = smf.ols('cig_iv.resids ~ np.log(rincome) + salestax + cigtax', data = c1995).fit()\n",
    "\n",
    "# Perform Wald test, use chi-square distribution\n",
    "cig_iv_OR_test = cig_iv_OR.wald_test([[0, 0, 1, 0],\n",
    "                                      [0, 0, 0, 1]], use_f=False, scalar=True)\n",
    "print('test: salestax = 0, cigtax = 0 \\n', cig_iv_OR_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrumental Variables Addtional Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Example: Return to Education for Married Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_ols_man: 0.10864865517467513\n",
      "\n",
      "b_iv_man: 0.05917347999936595\n",
      "\n",
      "table_ols: \n",
      "                b      se       t   pval\n",
      "Intercept -0.1852  0.1852 -0.9998  0.318\n",
      "educ       0.1086  0.0144  7.5451  0.000\n",
      "\n",
      "table_iv: \n",
      "                b      se       t    pval\n",
      "Intercept  0.4411  0.4461  0.9888  0.3233\n",
      "educ       0.0592  0.0351  1.6839  0.0929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note: You may need to first install install \"linearmodels\"\n",
    "# pip install linearmodels\n",
    "\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import linearmodels.iv as iv\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "mroz = woo.dataWoo('mroz')\n",
    "\n",
    "# restrict to non-missing wage observations:\n",
    "mroz = mroz.dropna(subset=['lwage'])\n",
    "\n",
    "cov_yz = np.cov(mroz['lwage'], mroz['fatheduc'])[1, 0]\n",
    "cov_xy = np.cov(mroz['educ'], mroz['lwage'])[1, 0]\n",
    "cov_xz = np.cov(mroz['educ'], mroz['fatheduc'])[1, 0]\n",
    "var_x = np.var(mroz['educ'], ddof=1)\n",
    "x_bar = np.mean(mroz['educ'])\n",
    "y_bar = np.mean(mroz['lwage'])\n",
    "\n",
    "# OLS slope parameter manually:\n",
    "b_ols_man = cov_xy / var_x\n",
    "print(f'b_ols_man: {b_ols_man}\\n')\n",
    "\n",
    "# IV slope parameter manually:\n",
    "b_iv_man = cov_yz / cov_xz\n",
    "print(f'b_iv_man: {b_iv_man}\\n')\n",
    "# OLS automatically:\n",
    "reg_ols = smf.ols(formula='np.log(wage) ~ educ', data=mroz)\n",
    "results_ols = reg_ols.fit()\n",
    "\n",
    "# print regression table:\n",
    "table_ols = pd.DataFrame({'b': round(results_ols.params, 4),\n",
    "                          'se': round(results_ols.bse, 4),\n",
    "                          't': round(results_ols.tvalues, 4),\n",
    "                          'pval': round(results_ols.pvalues, 4)})\n",
    "print(f'table_ols: \\n{table_ols}\\n')\n",
    "\n",
    "# IV automatically:\n",
    "reg_iv = iv.IV2SLS.from_formula(formula='np.log(wage) ~ 1 + [educ ~ fatheduc]',\n",
    "                                data=mroz)\n",
    "results_iv = reg_iv.fit(cov_type='unadjusted', debiased=True)\n",
    "\n",
    "# print regression table:\n",
    "table_iv = pd.DataFrame({'b': round(results_iv.params, 4),\n",
    "                         'se': round(results_iv.std_errors, 4),\n",
    "                         't': round(results_iv.tstats, 4),\n",
    "                         'pval': round(results_iv.pvalues, 4)})\n",
    "print(f'table_iv: \\n{table_iv}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Example: College Proximity as an IV for Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_redf: \n",
      "                     b      se        t    pval\n",
      "Intercept      16.6383  0.2406  69.1446  0.0000\n",
      "nearc4          0.3199  0.0879   3.6408  0.0003\n",
      "exper          -0.4125  0.0337 -12.2415  0.0000\n",
      "I(exper ** 2)   0.0009  0.0017   0.5263  0.5987\n",
      "black          -0.9355  0.0937  -9.9806  0.0000\n",
      "smsa            0.4022  0.1048   3.8372  0.0001\n",
      "south          -0.0516  0.1354  -0.3811  0.7032\n",
      "smsa66          0.0255  0.1058   0.2409  0.8096\n",
      "reg662         -0.0786  0.1871  -0.4203  0.6743\n",
      "reg663         -0.0279  0.1834  -0.1524  0.8789\n",
      "reg664          0.1172  0.2173   0.5394  0.5897\n",
      "reg665         -0.2726  0.2184  -1.2481  0.2121\n",
      "reg666         -0.3028  0.2371  -1.2773  0.2016\n",
      "reg667         -0.2168  0.2344  -0.9250  0.3550\n",
      "reg668          0.5239  0.2675   1.9587  0.0502\n",
      "reg669          0.2103  0.2025   1.0386  0.2991\n",
      "\n",
      "table_ols: \n",
      "                    b      se        t    pval\n",
      "Intercept      4.6208  0.0742  62.2476  0.0000\n",
      "educ           0.0747  0.0035  21.3510  0.0000\n",
      "exper          0.0848  0.0066  12.8063  0.0000\n",
      "I(exper ** 2) -0.0023  0.0003  -7.2232  0.0000\n",
      "black         -0.1990  0.0182 -10.9058  0.0000\n",
      "smsa           0.1364  0.0201   6.7851  0.0000\n",
      "south         -0.1480  0.0260  -5.6950  0.0000\n",
      "smsa66         0.0262  0.0194   1.3493  0.1773\n",
      "reg662         0.0964  0.0359   2.6845  0.0073\n",
      "reg663         0.1445  0.0351   4.1151  0.0000\n",
      "reg664         0.0551  0.0417   1.3221  0.1862\n",
      "reg665         0.1280  0.0418   3.0599  0.0022\n",
      "reg666         0.1405  0.0452   3.1056  0.0019\n",
      "reg667         0.1180  0.0448   2.6334  0.0085\n",
      "reg668        -0.0564  0.0513  -1.1010  0.2710\n",
      "reg669         0.1186  0.0388   3.0536  0.0023\n",
      "\n",
      "table_iv: \n",
      "                  b      se       t    pval\n",
      "Intercept    3.6662  0.9248  3.9641  0.0001\n",
      "exper        0.1083  0.0237  4.5764  0.0000\n",
      "I(exper**2) -0.0023  0.0003 -7.0014  0.0000\n",
      "black       -0.1468  0.0539 -2.7231  0.0065\n",
      "smsa         0.1118  0.0317  3.5313  0.0004\n",
      "south       -0.1447  0.0273 -5.3023  0.0000\n",
      "smsa66       0.0185  0.0216  0.8576  0.3912\n",
      "reg662       0.1008  0.0377  2.6739  0.0075\n",
      "reg663       0.1483  0.0368  4.0272  0.0001\n",
      "reg664       0.0499  0.0437  1.1408  0.2541\n",
      "reg665       0.1463  0.0471  3.1079  0.0019\n",
      "reg666       0.1629  0.0519  3.1382  0.0017\n",
      "reg667       0.1346  0.0494  2.7240  0.0065\n",
      "reg668      -0.0831  0.0593 -1.4002  0.1616\n",
      "reg669       0.1078  0.0418  2.5784  0.0100\n",
      "educ         0.1315  0.0550  2.3926  0.0168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import linearmodels.iv as iv\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "card = woo.dataWoo('card')\n",
    "\n",
    "# checking for relevance with reduced form:\n",
    "reg_redf = smf.ols(\n",
    "    formula='educ ~ nearc4 + exper + I(exper**2) + black + smsa +'\n",
    "    'south + smsa66 + reg662 + reg663 + reg664 + reg665 + reg666 +'\n",
    "    'reg667 + reg668 + reg669', data=card)\n",
    "results_redf = reg_redf.fit()\n",
    "\n",
    "# print regression table:\n",
    "table_redf = pd.DataFrame({'b': round(results_redf.params, 4),\n",
    "                           'se': round(results_redf.bse, 4),\n",
    "                           't': round(results_redf.tvalues, 4),\n",
    "                           'pval': round(results_redf.pvalues, 4)})\n",
    "print(f'table_redf: \\n{table_redf}\\n')\n",
    "\n",
    "# OLS:\n",
    "reg_ols = smf.ols(\n",
    "    formula='np.log(wage) ~ educ + exper + I(exper**2) + black + smsa +'\n",
    "    'south + smsa66 + reg662 + reg663 + reg664 + reg665 +'\n",
    "    'reg666 + reg667 + reg668 + reg669', data=card)\n",
    "results_ols = reg_ols.fit()\n",
    "\n",
    "# print regression table:\n",
    "table_ols = pd.DataFrame({'b': round(results_ols.params, 4),\n",
    "                          'se': round(results_ols.bse, 4),\n",
    "                          't': round(results_ols.tvalues, 4),\n",
    "                          'pval': round(results_ols.pvalues, 4)})\n",
    "print(f'table_ols: \\n{table_ols}\\n')\n",
    "\n",
    "# IV automatically:\n",
    "reg_iv = iv.IV2SLS.from_formula(\n",
    "    formula='np.log(wage)~ 1 + exper + I(exper**2) + black + smsa + '\n",
    "            'south + smsa66 + reg662 + reg663 + reg664 + reg665 +'\n",
    "            'reg666 + reg667 + reg668 + reg669 + [educ ~ nearc4]',\n",
    "    data=card)\n",
    "results_iv = reg_iv.fit(cov_type='unadjusted', debiased=True)\n",
    "\n",
    "# print regression table:\n",
    "table_iv = pd.DataFrame({'b': round(results_iv.params, 4),\n",
    "                         'se': round(results_iv.std_errors, 4),\n",
    "                         't': round(results_iv.tstats, 4),\n",
    "                         'pval': round(results_iv.pvalues, 4)})\n",
    "print(f'table_iv: \\n{table_iv}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Stage Least Squares Additional Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Return to Education for Married Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_redf: \n",
      "                    b      se        t    pval\n",
      "Intercept      9.1026  0.4266  21.3396  0.0000\n",
      "exper          0.0452  0.0403   1.1236  0.2618\n",
      "I(exper ** 2) -0.0010  0.0012  -0.8386  0.4022\n",
      "motheduc       0.1576  0.0359   4.3906  0.0000\n",
      "fatheduc       0.1895  0.0338   5.6152  0.0000\n",
      "\n",
      "table_secstg: \n",
      "                    b      se       t    pval\n",
      "Intercept      0.0481  0.4198  0.1146  0.9088\n",
      "educ_fitted    0.0614  0.0330  1.8626  0.0632\n",
      "exper          0.0442  0.0141  3.1361  0.0018\n",
      "I(exper ** 2) -0.0009  0.0004 -2.1344  0.0334\n",
      "\n",
      "table_iv: \n",
      "                  b      se       t    pval\n",
      "Intercept    0.0481  0.4003  0.1202  0.9044\n",
      "exper        0.0442  0.0134  3.2883  0.0011\n",
      "I(exper**2) -0.0009  0.0004 -2.2380  0.0257\n",
      "educ         0.0614  0.0314  1.9530  0.0515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import linearmodels.iv as iv\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "mroz = woo.dataWoo('mroz')\n",
    "\n",
    "# restrict to non-missing wage observations:\n",
    "mroz = mroz.dropna(subset=['lwage'])\n",
    "\n",
    "# 1st stage (reduced form):\n",
    "reg_redf = smf.ols(formula='educ ~ exper + I(exper**2) + motheduc + fatheduc',\n",
    "                   data=mroz)\n",
    "results_redf = reg_redf.fit()\n",
    "mroz['educ_fitted'] = results_redf.fittedvalues\n",
    "\n",
    "# print regression table:\n",
    "table_redf = pd.DataFrame({'b': round(results_redf.params, 4),\n",
    "                           'se': round(results_redf.bse, 4),\n",
    "                           't': round(results_redf.tvalues, 4),\n",
    "                           'pval': round(results_redf.pvalues, 4)})\n",
    "print(f'table_redf: \\n{table_redf}\\n')\n",
    "\n",
    "# 2nd stage:\n",
    "reg_secstg = smf.ols(formula='np.log(wage) ~ educ_fitted + exper + I(exper**2)',\n",
    "                     data=mroz)\n",
    "results_secstg = reg_secstg.fit()\n",
    "\n",
    "# print regression table:\n",
    "table_secstg = pd.DataFrame({'b': round(results_secstg.params, 4),\n",
    "                             'se': round(results_secstg.bse, 4),\n",
    "                             't': round(results_secstg.tvalues, 4),\n",
    "                             'pval': round(results_secstg.pvalues, 4)})\n",
    "print(f'table_secstg: \\n{table_secstg}\\n')\n",
    "\n",
    "# IV automatically:\n",
    "reg_iv = iv.IV2SLS.from_formula(\n",
    "    formula='np.log(wage) ~ 1 + exper + I(exper**2) +'\n",
    "            '[educ  ~ motheduc + fatheduc]',\n",
    "    data=mroz)\n",
    "results_iv = reg_iv.fit(cov_type='unadjusted', debiased=True)\n",
    "\n",
    "# print regression table:\n",
    "table_iv = pd.DataFrame({'b': round(results_iv.params, 4),\n",
    "                         'se': round(results_iv.std_errors, 4),\n",
    "                         't': round(results_iv.tstats, 4),\n",
    "                         'pval': round(results_iv.pvalues, 4)})\n",
    "print(f'table_iv: \\n{table_iv}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing  for Exogeneity of the Regressors Additional Example\n",
    "## Example: Return to Education for Married Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "table_secstg: \n",
      "                    b      se       t    pval\n",
      "Intercept      0.0481  0.3946  0.1219  0.9030\n",
      "resid          0.0582  0.0348  1.6711  0.0954\n",
      "educ           0.0614  0.0310  1.9815  0.0482\n",
      "exper          0.0442  0.0132  3.3363  0.0009\n",
      "I(exper ** 2) -0.0009  0.0004 -2.2706  0.0237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "mroz = woo.dataWoo('mroz')\n",
    "\n",
    "# restrict to non-missing wage observations:\n",
    "mroz = mroz.dropna(subset=['lwage'])\n",
    "\n",
    "# 1st stage (reduced form):\n",
    "reg_redf = smf.ols(formula='educ ~ exper + I(exper**2) + motheduc + fatheduc',\n",
    "                   data=mroz)\n",
    "results_redf = reg_redf.fit()\n",
    "mroz['resid'] = results_redf.resid\n",
    "\n",
    "# 2nd stage:\n",
    "reg_secstg = smf.ols(formula='np.log(wage)~ resid + educ + exper + I(exper**2)',\n",
    "                     data=mroz)\n",
    "results_secstg = reg_secstg.fit()\n",
    "\n",
    "# print regression table:\n",
    "table_secstg = pd.DataFrame({'b': round(results_secstg.params, 4),\n",
    "                             'se': round(results_secstg.bse, 4),\n",
    "                             't': round(results_secstg.tvalues, 4),\n",
    "                             'pval': round(results_secstg.pvalues, 4)})\n",
    "print(f'table_secstg: \\n{table_secstg}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Overidentifying Restrictions Additional Example\n",
    "## Example: Return to Education for Married Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_iv: \n",
      "                  b      se       t    pval\n",
      "Intercept    0.0481  0.4003  0.1202  0.9044\n",
      "exper        0.0442  0.0134  3.2883  0.0011\n",
      "I(exper**2) -0.0009  0.0004 -2.2380  0.0257\n",
      "educ         0.0614  0.0314  1.9530  0.0515\n",
      "\n",
      "r2: 0.0008833444088024445\n",
      "\n",
      "n: 428.0\n",
      "\n",
      "teststat: 0.37807140696744623\n",
      "\n",
      "pval: 0.5386371981603848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import linearmodels.iv as iv\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "\n",
    "mroz = woo.dataWoo('mroz')\n",
    "\n",
    "# restrict to non-missing wage observations:\n",
    "mroz = mroz.dropna(subset=['lwage'])\n",
    "\n",
    "# IV regression:\n",
    "reg_iv = iv.IV2SLS.from_formula(formula='np.log(wage) ~ 1 + exper + I(exper**2) +'\n",
    "                                        '[educ ~ motheduc + fatheduc]', data=mroz)\n",
    "results_iv = reg_iv.fit(cov_type='unadjusted', debiased=True)\n",
    "\n",
    "# print regression table:\n",
    "table_iv = pd.DataFrame({'b': round(results_iv.params, 4),\n",
    "                         'se': round(results_iv.std_errors, 4),\n",
    "                         't': round(results_iv.tstats, 4),\n",
    "                         'pval': round(results_iv.pvalues, 4)})\n",
    "print(f'table_iv: \\n{table_iv}\\n')\n",
    "\n",
    "# auxiliary regression:\n",
    "mroz['resid_iv'] = results_iv.resids\n",
    "reg_aux = smf.ols(formula='resid_iv ~ exper + I(exper**2) + motheduc + fatheduc',\n",
    "                  data=mroz)\n",
    "results_aux = reg_aux.fit()\n",
    "\n",
    "# calculations for test:\n",
    "r2 = results_aux.rsquared\n",
    "n = results_aux.nobs\n",
    "teststat = n * r2\n",
    "pval = 1 - stats.chi2.cdf(teststat, 1)\n",
    "\n",
    "print(f'r2: {r2}\\n')\n",
    "print(f'n: {n}\\n')\n",
    "print(f'teststat: {teststat}\\n')\n",
    "print(f'pval: {pval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV with Panel Data Additional Example\n",
    "## Example: Job Training and Worker Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_iv: \n",
      "                   b      se       t    pval\n",
      "Intercept    -0.0635  0.1178 -0.5388  0.5960\n",
      "hrsemp_diff1 -0.0129  0.0093 -1.3833  0.1818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import linearmodels.iv as iv\n",
    "\n",
    "jtrain = woo.dataWoo('jtrain')\n",
    "\n",
    "# define panel data (for 1987 and 1988 only):\n",
    "jtrain_87_88 = jtrain.loc[(jtrain['year'] == 1987) | (jtrain['year'] == 1988), :]\n",
    "jtrain_87_88 = jtrain_87_88.set_index(['fcode', 'year'])\n",
    "\n",
    "# manual computation of deviations of entity means:\n",
    "jtrain_87_88['lscrap_diff1'] = jtrain_87_88.sort_values(['fcode', 'year']).groupby('fcode')['lscrap'].diff()\n",
    "jtrain_87_88['hrsemp_diff1'] = jtrain_87_88.sort_values(['fcode', 'year']).groupby('fcode')['hrsemp'].diff()\n",
    "jtrain_87_88['grant_diff1'] = jtrain_87_88.sort_values(['fcode', 'year']).groupby('fcode')['grant'].diff()\n",
    "jtrain_87_88 = jtrain_87_88.dropna()\n",
    "\n",
    "# IV regression:\n",
    "reg_iv = iv.IV2SLS.from_formula(formula='lscrap_diff1 ~ 1 + [hrsemp_diff1 ~ grant_diff1]',data=jtrain_87_88)\n",
    "results_iv = reg_iv.fit(cov_type='unadjusted', debiased=True)\n",
    "\n",
    "# print regression table:\n",
    "table_iv = pd.DataFrame({'b': round(results_iv.params, 4),\n",
    "                         'se': round(results_iv.std_errors, 4),\n",
    "                         't': round(results_iv.tstats, 4),\n",
    "                         'pval': round(results_iv.pvalues, 4)})\n",
    "print(f'table_iv: \\n{table_iv}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
